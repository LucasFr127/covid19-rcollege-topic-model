{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237d33a7",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a661d2",
   "metadata": {},
   "source": [
    "The data collected in the first step contains various types of noise. It's important to remove the noise to produce appropriate document embeddings. In order to clean the data, a series of regular expressions are used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress FutureWarning messages\n",
    "# PS: to suppress these messages it's necessary to keep this at the beginning of the notebook\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versions used:\n",
    "#\n",
    "# python     3.9.7\n",
    "# emoji      1.6.1\n",
    "# matplotlib 3.5.0\n",
    "# pandas     1.3.5\n",
    "\n",
    "import datetime as dt\n",
    "import html\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3afb5c7",
   "metadata": {},
   "source": [
    "First, we need to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6faa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the DataFrame that was saved in the first notebook\n",
    "posts_df = pd.read_csv('./rcollege_20200101-20220101_praw.csv')\n",
    "\n",
    "# Return the first 5 rows\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ab772",
   "metadata": {},
   "source": [
    "Then, we reorganize the data and remove unneeded rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7306f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant columns\n",
    "posts_df = posts_df[['id', 'created_utc', 'title', 'selftext', 'author', 'score']]\n",
    "\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc27260",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df['created_utc'] = pd.to_datetime(posts_df['created_utc'], unit='s')\n",
    "posts_df['created_utc'] = posts_df['created_utc'].dt.date\n",
    "\n",
    "posts_df['created_utc'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17046f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "posts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove posts with duplicate ids\n",
    "posts_df = posts_df.drop_duplicates(subset='id')\n",
    "\n",
    "# Remove posts that contain NA\n",
    "posts_df = posts_df.dropna(subset=['id'])\n",
    "posts_df = posts_df.dropna(subset=['created_utc'])\n",
    "posts_df = posts_df.dropna(subset=['title'])\n",
    "posts_df = posts_df.dropna(subset=['selftext'])\n",
    "posts_df = posts_df.dropna(subset=['author'])\n",
    "posts_df = posts_df.dropna(subset=['score'])\n",
    "\n",
    "# Remove posts where author is [deleted]\n",
    "posts_df = posts_df[posts_df.author != '[deleted]']\n",
    "\n",
    "# Remove posts where title is [deleted by user]\n",
    "posts_df = posts_df[posts_df.title != '[deleted by user]']\n",
    "\n",
    "# Remove posts where selftext is [deleted] or [removed]\n",
    "posts_df = posts_df[posts_df.selftext != '[deleted]']\n",
    "posts_df = posts_df[posts_df.selftext != '[removed]']\n",
    "\n",
    "# Remove posts where title and selftext are both duplicates\n",
    "posts_df.drop_duplicates(subset=['title', 'selftext'], inplace=True)\n",
    "\n",
    "posts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1483630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dtypes\n",
    "posts_df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0485b",
   "metadata": {},
   "source": [
    "Afterwards, we can remove polls, unescape HTML entities and remove emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9230ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove polls\n",
    "posts_df['selftext'] = posts_df['selftext'].map(\n",
    "    lambda x: re.sub(r'\\[removed\\]\\s+\\[View Poll\\]\\(https:\\/\\/www.reddit.com\\/poll\\/[A-Za-z0-9]+\\)', '', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(\n",
    "    lambda x: re.sub(r'\\[View Poll\\]\\(https:\\/\\/www.reddit.com\\/poll\\/[A-Za-z0-9]+\\)', '', x))\n",
    "\n",
    "posts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove posts where selftext is \\n\\n\n",
    "posts_df = posts_df[posts_df.selftext != '\\n\\n']\n",
    "\n",
    "posts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b92f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unescape HTML entities in title\n",
    "posts_df['title'] = posts_df['title'].map(lambda x: html.unescape(x))\n",
    "\n",
    "# Unescape HTML entities in selftext\n",
    "# PS: when &amp;amp;amp;#x200B; is present it's necessary to run this 4 times\n",
    "for _ in range(3):\n",
    "    posts_df['selftext'] = posts_df['selftext'].map(lambda x: html.unescape(x))\n",
    "\n",
    "# Remove remaining &#x200B;\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub('&#x200B;', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8be51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove emojis\n",
    "posts_df['title'] = posts_df['title'].map(lambda x: re.sub(emoji.get_emoji_regexp(), ' ', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(emoji.get_emoji_regexp(), ' ', x))\n",
    "\n",
    "posts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7d0b7",
   "metadata": {},
   "source": [
    "A series of regular expressions are then called to clean Markdown syntax and other types of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove newlines\n",
    "# posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\n+', ' ', x).strip())\n",
    "# posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\s\\s+', ' ', x))\n",
    "\n",
    "# Remove syntax for quote block\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'^\\\\>', '', x, flags=re.MULTILINE))\n",
    "\n",
    "# Remove syntax for heading\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'^#\\s|^##\\s', '', x, flags=re.MULTILINE))\n",
    "\n",
    "# Replace \\# in the beginning of a line with #\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'^\\\\#', '#', x, flags=re.MULTILINE))\n",
    "\n",
    "# Replace \\* with a temporary code\n",
    "# Where code = (asterisk + random combination of 8 characters)\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\\\\\*', '(asterisk=s&RDaeK)', x))\n",
    "\n",
    "# Remove syntax for bold, italic and pointed list\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\*', '', x))\n",
    "\n",
    "# Replace the temporary code above with *\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\(asterisk=s&RDaeK\\)', '*', x))\n",
    "\n",
    "# Remove space after open parenthesis or before close parenthesis\n",
    "posts_df['title'] = posts_df['title'].map(lambda x: re.sub(r'\\( ', '(', x))\n",
    "posts_df['title'] = posts_df['title'].map(lambda x: re.sub(r' \\)', ')', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\( ', '(', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r' \\)', ')', x))\n",
    "\n",
    "# Remove space after open bracket or before close bracket\n",
    "posts_df['title'] = posts_df['title'].map(lambda x: re.sub(r'\\[ ', '[', x))\n",
    "posts_df['title'] = posts_df['title'].map(lambda x: re.sub(r' \\]', ']', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\[ ', '[', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r' \\]', ']', x))\n",
    "\n",
    "# Remove clickable e-mails\n",
    "posts_df['selftext'] = posts_df['selftext'].map(\n",
    "    lambda x: re.sub(r'\\[\\]\\(mailto:([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})\\)', '', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(\n",
    "    lambda x: re.sub(r'\\[\"*?([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})\\]\\(mailto:\"*?([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})\\)', '', x))\n",
    "\n",
    "# Remove clickable links, while mantaining the text\n",
    "posts_df['title'] = posts_df['title'].map(\n",
    "    lambda x: re.sub(r'[^0-9\\D]?\\[([^\\[\\s].*?)?\\]\\(([A-Za-z0-9]*?(:\\/*)?(www[0-9]?\\.)?[A-Za-z0-9\\-]*(\\.[A-Za-z0-9%-_]*)?\\.[A-Za-z0-9]+\\/?(\\/[#-_a-~À-ÖØ-öø-ÿ™]+)*)\\)[^0-9\\D]?', r'\\1', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(\n",
    "    lambda x: re.sub(r'[^0-9\\D]?\\[([^\\[\\s].*?)?\\]\\(([A-Za-z0-9]*?(:\\/*)?(www[0-9]?\\.)?[A-Za-z0-9\\-]*(\\.[A-Za-z0-9%-_]*)?\\.[A-Za-z0-9]+\\/?(\\/[#-_a-~À-ÖØ-öø-ÿ™]+)*)\\)[^0-9\\D]?', r'\\1', x))\n",
    "\n",
    "# Remove syntax for strikethrough\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub('~~(.*?)~~', r'\\1', x))\n",
    "\n",
    "# Remove links\n",
    "posts_df['selftext'] = posts_df['selftext'].map(\n",
    "    lambda x: re.sub(r'[A-Za-z0-9]+:\\/\\/[A-Za-z0-9%-_]+(\\/[A-Za-z0-9%-_])*(#|\\\\?)[#-_a-~À-ÖØ-öø-ÿ™]*[^)]', '', x))\n",
    "\n",
    "# Remove e-mails\n",
    "posts_df['title'] = posts_df['title'].map(\n",
    "    lambda x: re.sub('([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})', '', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(\n",
    "    lambda x: re.sub('([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})', '', x))\n",
    "\n",
    "# Remove syntax for spoiler\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub('>!(.*?)!<', r'\\1', x))\n",
    "\n",
    "# Remove syntax for inline code and code block\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub('`', '', x))\n",
    "\n",
    "# Remove emotes :-) :-( :-/\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r':-\\)|:-\\(|:-\\/', '', x))\n",
    "\n",
    "# Remove emote T^T\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'T\\^T', '', x))\n",
    "\n",
    "# Remove syntax for superscript\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\^\\((.*?)\\)', r'\\1', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\^', '', x))\n",
    "\n",
    "# Remove syntax for table\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\|', ' ', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(':-', '', x))\n",
    "\n",
    "# Replace \\[ with [ and \\] with ]\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\\\\\[', '[', x))\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(r'\\\\\\]', ']', x))\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "posts_df['selftext'] = posts_df['selftext'].map(lambda x: re.sub(' +', ' ', x).strip())\n",
    "\n",
    "posts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f764551",
   "metadata": {},
   "source": [
    "Finally, the leftover noise can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove posts with empty selftext\n",
    "posts_df = posts_df[posts_df.selftext != '']\n",
    "posts_df = posts_df[posts_df.selftext != '\\u200b']\n",
    "\n",
    "# Remove posts where selftext is [deleted] or [removed]\n",
    "# posts_df = posts_df[posts_df.selftext != '[deleted]']\n",
    "# posts_df = posts_df[posts_df.selftext != '[removed]']\n",
    "\n",
    "# Return the most common value for selftext\n",
    "# This is useful for finding entities such as \\u200b\n",
    "# posts_df['selftext'].value_counts().idxmax()\n",
    "\n",
    "posts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8f007",
   "metadata": {},
   "source": [
    "Now, the data will be reorganized and stored in a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff4dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row of the DataFrame, join title and selftext\n",
    "# We'll call this column \"document\"\n",
    "posts_df['document'] = posts_df['title'] + '\\n' + posts_df['selftext']\n",
    "\n",
    "# Keep only the columns created_utc, document and score\n",
    "posts_df = posts_df[['created_utc', 'document', 'score']]\n",
    "\n",
    "posts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data\n",
    "posts_df.to_csv('./rcollege_clean_20200101-20220101_praw.csv', columns=list(posts_df.axes[1]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4d7dd",
   "metadata": {},
   "source": [
    "Lastly, we can visualize the approximate number of tokens per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c91056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt_x = posts_df['document'].str.split().map(lambda x: len(x))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.xlabel(\"Número de $\\it{Tokens}$\", fontsize=10)\n",
    "plt.ylabel(\"Número de Documentos\", fontsize=10)\n",
    "\n",
    "plt.hist(plt_x, color='xkcd:bluey purple', bins=range(0, 768))\n",
    "\n",
    "plt.axvline(plt_x.mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "plt.text(plt_x.mean()*1.1, max_ylim*0.9, \"Média: {:.2f}\".format(plt_x.mean()))\n",
    "\"\"\"\n",
    "\n",
    "plt_x = posts_df['document'].str.split().map(lambda x: len(x))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.xlabel(\"Number of Tokens\", fontsize=10)\n",
    "plt.ylabel(\"Number of Documents\", fontsize=10)\n",
    "\n",
    "plt.hist(plt_x, color='xkcd:bluey purple', bins=range(0, 768))\n",
    "\n",
    "plt.axvline(plt_x.mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "plt.text(plt_x.mean()*1.1, max_ylim*0.9, \"Mean: {:.2f}\".format(plt_x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d73ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "j0 = [i for i in list(plt_x) if i <= 128]\n",
    "p0 = 100 * len(j0)/len(plt_x)\n",
    "\n",
    "j1 = [i for i in list(plt_x) if i <= 256]\n",
    "p1 = 100 * len(j1)/len(plt_x)\n",
    "\n",
    "j2 = [i for i in list(plt_x) if i <= 384]\n",
    "p2 = 100 * len(j2)/len(plt_x)\n",
    "\n",
    "j3 = [i for i in list(plt_x) if i <= 512]\n",
    "p3 = 100 * len(j3)/len(plt_x)\n",
    "\n",
    "print(\"{0:.2f} percent of the documents contain up to 128 tokens.\".format(p0))\n",
    "print(\"{0:.2f} percent of the documents contain up to 256 tokens.\".format(p1))\n",
    "print(\"{0:.2f} percent of the documents contain up to 384 tokens.\".format(p2))\n",
    "print(\"{0:.2f} percent of the documents contain up to 512 tokens.\".format(p3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
