{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33bb6af",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versions used:\n",
    "#\n",
    "# python       3.9.7\n",
    "# fasttext     0.9.2\n",
    "# matplotlib   3.5.0\n",
    "# numpy        1.20.3\n",
    "# pandas       1.3.5\n",
    "# bertopic     0.9.4\n",
    "# gensim       4.1.2\n",
    "# hdbscan      0.8.27\n",
    "# scikit-learn 1.0.2\n",
    "# tqdm         4.62.3\n",
    "# umap-learn   0.5.2\n",
    "\n",
    "import datetime as dt\n",
    "import re\n",
    "import requests as req\n",
    "from types import MethodType\n",
    "from typing import List, Union\n",
    "\n",
    "import fasttext.util\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic._utils import check_documents_type, check_is_fitted\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "\n",
    "from topic_model_diversity.diversity_metrics import irbo, word_embedding_irbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221e82a",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords list from Stanford CoreNLP\n",
    "url = 'https://raw.githubusercontent.com/stanfordnlp/CoreNLP/main/data/edu/stanford/nlp/patterns/surface/stopwords.txt'\n",
    "res = req.get(url)\n",
    "\n",
    "# Save stopwords list to a local TXT file\n",
    "file = open('./stopwords.txt', 'w')\n",
    "file.write(res.text)\n",
    "file.close()\n",
    "\n",
    "# Download fastText model\n",
    "# PS: note that will require a lot of disk space (~12GB)\n",
    "fasttext.util.download_model('en', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d320c",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file containing timestamps, docs and upvotes\n",
    "posts_df = pd.read_csv('./rcollege_clean_20200101-20220101_praw.csv')\n",
    "\n",
    "# For posts_df, save timestamps, docs and upvotes to separate lists\n",
    "timestamps = posts_df.created_utc.to_list()\n",
    "docs = posts_df.document.to_list()\n",
    "upvotes = posts_df.score.to_list()\n",
    "\n",
    "# Order rows by date\n",
    "posts_df = posts_df.set_index(posts_df['created_utc'])\n",
    "posts_df = posts_df.sort_index()\n",
    "\n",
    "# Create a DataFrame for each year\n",
    "# Where y1_df corresponds to 2020, and y2_df to 2021\n",
    "y1_df = posts_df['2020-01-01':'2020-12-31']\n",
    "y2_df = posts_df['2021-01-01':]\n",
    "\n",
    "# For y1_df, save timestamps, docs and upvotes to separate lists\n",
    "y1_timestamps = y1_df.created_utc.to_list()\n",
    "y1_docs = y1_df.document.to_list()\n",
    "y1_upvotes = y1_df.score.to_list()\n",
    "\n",
    "# For y2_df, save timestamps, docs and upvotes to separate lists\n",
    "y2_timestamps = y2_df.created_utc.to_list()\n",
    "y2_docs = y2_df.document.to_list()\n",
    "y2_upvotes = y2_df.score.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a432982",
   "metadata": {},
   "source": [
    "## Defining stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stop_words list and populate it with initial values\n",
    "stop_words = [\"aren\", \"couldn\", \"didn\", \"doesn\",\n",
    "              \"don\", \"hadn\", \"hasn\", \"haven\",\n",
    "              \"isn\", \"mustn\", \"shan\", \"shouldn\",\n",
    "              \"wasn\", \"weren\", \"won\", \"wouldn\"]\n",
    "\n",
    "# Open the TXT file saved two steps above\n",
    "file = open('./stopwords.txt','r')\n",
    "\n",
    "# Append the words from the file to the stop_words list defined above\n",
    "for line in file:\n",
    "    stop_words.append(line.strip())\n",
    "\n",
    "# Close the TXT file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca6677",
   "metadata": {},
   "source": [
    "## Defining UMAP, HDBSCAN and CountVectorizer\n",
    "\n",
    "https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html\n",
    "\n",
    "https://umap-learn.readthedocs.io/en/latest/parameters.html  \n",
    "https://umap-learn.readthedocs.io/en/latest/reproducibility.html\n",
    "\n",
    "https://hdbscan.readthedocs.io/en/latest/parameter_selection.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22788a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UMAP parameters\n",
    "umap_model = UMAP(n_neighbors=28,  # Default: 15\n",
    "                  n_components=12,  # Default: 5\n",
    "                  metric='cosine',\n",
    "                  min_dist=0.0,\n",
    "                  random_state=42)\n",
    "\n",
    "# Define HDBSCAN parameters\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=17,  # Equals to min_topic_size (Default: 10)\n",
    "                        metric='euclidean',\n",
    "                        cluster_selection_method='eom',\n",
    "                        prediction_data=True)\n",
    "\n",
    "# Define CountVectorizer parameters\n",
    "vectorizer_model = CountVectorizer(tokenizer=None,\n",
    "                                   stop_words=stop_words,\n",
    "                                   ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a2079",
   "metadata": {},
   "source": [
    "## Topic Modeling with BERTopic\n",
    "\n",
    "https://maartengr.github.io/BERTopic/api/bertopic.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5f0be",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3190f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define BERTopic parameters and fit the model\n",
    "topic_model = BERTopic(language='english',\n",
    "                       top_n_words=10,\n",
    "                       min_topic_size=17,  # Default: 10\n",
    "                       nr_topics=None,\n",
    "                       verbose=True,\n",
    "                       embedding_model='all-MiniLM-L6-v2',  # Default: all-MiniLM-L6-v2\n",
    "                       umap_model=umap_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       vectorizer_model=vectorizer_model).fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for docs\n",
    "topics, probs = topic_model.transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09210756",
   "metadata": {},
   "source": [
    "### Extracting topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e667b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for y1_docs\n",
    "y1_topics, y1_probs = topic_model.transform(y1_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for y2_docs\n",
    "y2_topics, y2_probs = topic_model.transform(y2_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b395a",
   "metadata": {},
   "source": [
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9483d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3523a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "\n",
    "freq.to_csv('./vis_freq_all.csv', columns=list(freq.axes[1]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9bf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_reps = freq\n",
    "list_reps = [-1]\n",
    "\n",
    "for i in range(161):\n",
    "    reps = topic_model.get_representative_docs(topic=i)\n",
    "    list_reps.append(reps)\n",
    "\n",
    "freq_reps[\"Representative Docs\"] = list_reps\n",
    "\n",
    "freq_reps.to_csv('./vis_freq_reps_all.csv', columns=list(freq_reps.axes[1]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tops = pd.read_csv('./vis_freq_reps_all.csv')\n",
    "list_tops = [-1]\n",
    "\n",
    "for i in range(161):\n",
    "    tops = topic_model.get_topic(i)\n",
    "    list_tops.append(tops)\n",
    "\n",
    "freq_tops[\"Words\"] = list_tops\n",
    "\n",
    "freq_tops.to_csv('./vis_freq_tops_all.csv', columns=list(freq_tops.axes[1]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs_df = pd.DataFrame(list(zip(topics, timestamps, docs, upvotes)), columns=[\"Topic\", \"Date\", \"Document\", \"Upvotes\"])\n",
    "\n",
    "topic_docs_df.to_csv('./vis_topic_docs_all.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ccc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs_df = topic_docs_df.set_index(topic_docs_df[\"Date\"])\n",
    "topic_docs_df = topic_docs_df.sort_index()\n",
    "\n",
    "topic_docs_37_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 37]\n",
    "topic_docs_37_df.to_csv('./vis_topic_docs_37.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_64_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 64]\n",
    "topic_docs_64_df.to_csv('./vis_topic_docs_64.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_100_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 100]\n",
    "topic_docs_100_df.to_csv('./vis_topic_docs_100.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_106_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 106]\n",
    "topic_docs_106_df.to_csv('./vis_topic_docs_106.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_123_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 123]\n",
    "topic_docs_123_df.to_csv('./vis_topic_docs_123.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_127_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 127]\n",
    "topic_docs_127_df.to_csv('./vis_topic_docs_127.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_139_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 139]\n",
    "topic_docs_139_df.to_csv('./vis_topic_docs_139.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_141_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 141]\n",
    "topic_docs_141_df.to_csv('./vis_topic_docs_141.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_156_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 156]\n",
    "topic_docs_156_df.to_csv('./vis_topic_docs_156.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_159_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 159]\n",
    "topic_docs_159_df.to_csv('./vis_topic_docs_159.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)\n",
    "\n",
    "topic_docs_160_df = topic_docs_df.loc[topic_docs_df[\"Topic\"] == 160]\n",
    "topic_docs_160_df.to_csv('./vis_topic_docs_160.csv', columns=list(topic_docs_df.axes[1]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd844fe2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "freq.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(topic=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f70525",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"posture\", top_n=5)\n",
    "\n",
    "topic_model.get_topic(similar_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b54fd",
   "metadata": {},
   "source": [
    "# Topic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24e75a",
   "metadata": {},
   "source": [
    "### Normalized Pointwise Mutual Information (NPMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process documents before topic evaluation\n",
    "documents = pd.DataFrame({'Document': docs,\n",
    "                          'ID': range(len(docs)),\n",
    "                          'Topic': topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for topic coherence evaluation\n",
    "words = vectorizer.get_feature_names()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Define CoherenceModel parameters\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "\n",
    "# Calculate topic coherence\n",
    "coherence = coherence_model.get_coherence()\n",
    "\n",
    "coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c485388",
   "metadata": {},
   "source": [
    "### Word Embedding-based Inverted Rank-Biased Overlap (WE-IRBO)\n",
    "\n",
    "https://radimrehurek.com/gensim/models/fasttext.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = len(topic_model.get_topic_info().index) - 1\n",
    "\n",
    "word_list = []\n",
    "\n",
    "for topic in topic_list:\n",
    "        words = [word for word, _ in topic_model.get_topic(topic)][:10]\n",
    "        word_list.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fastText embeddings\n",
    "# PS: note that this can take a while and use a lot of memory\n",
    "wv = load_facebook_vectors('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate topic diversity with WE-IRBO\n",
    "word_embedding_irbo(word_list, wv, weight=0.9, topk=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1338c7",
   "metadata": {},
   "source": [
    "## Defining an Alternative Frequency for topics_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2898c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alternative topics_over_time where frequency is calculated based on upvotes\n",
    "def topics_over_time_upvotes(self,\n",
    "                             docs: List[str],\n",
    "                             topics: List[int],\n",
    "                             timestamps: Union[List[str],\n",
    "                                               List[int]],\n",
    "                             upvotes: List[int],\n",
    "                             nr_bins: int = None,\n",
    "                             datetime_format: str = None,\n",
    "                             evolution_tuning: bool = True,\n",
    "                             global_tuning: bool = True) -> pd.DataFrame:\n",
    "    \"\"\" Create topics over time\n",
    "\n",
    "        To create the topics over time, BERTopic needs to be already fitted once.\n",
    "        From the fitted models, the c-TF-IDF representations are calculate at\n",
    "        each timestamp t. Then, the c-TF-IDF representations at timestamp t are\n",
    "        averaged with the global c-TF-IDF representations in order to fine-tune the\n",
    "        local representations.\n",
    "\n",
    "    NOTE:\n",
    "        Make sure to use a limited number of unique timestamps (<100) as the\n",
    "        c-TF-IDF representation will be calculated at each single unique timestamp.\n",
    "        Having a large number of unique timestamps can take some time to be calculated.\n",
    "        Moreover, there aren't many use-cased where you would like to see the difference\n",
    "        in topic representations over more than 100 different timestamps.\n",
    "\n",
    "    Arguments:\n",
    "        docs: The documents you used when calling either `fit` or `fit_transform`\n",
    "        topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
    "        timestamps: The timestamp of each document. This can be either a list of strings or ints.\n",
    "                    If it is a list of strings, then the datetime format will be automatically\n",
    "                    inferred. If it is a list of ints, then the documents will be ordered by\n",
    "                    ascending order.\n",
    "        nr_bins: The number of bins you want to create for the timestamps. The left interval will\n",
    "                 be chosen as the timestamp. An additional column will be created with the\n",
    "                 entire interval.\n",
    "        datetime_format: The datetime format of the timestamps if they are strings, eg “%d/%m/%Y”.\n",
    "                         Set this to None if you want to have it automatically detect the format.\n",
    "                         See strftime documentation for more information on choices:\n",
    "                         https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n",
    "        evolution_tuning: Fine-tune each topic representation at timestamp t by averaging its\n",
    "                          c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates\n",
    "                          evolutionary topic representations.\n",
    "        global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix\n",
    "                   with the global c-TF-IDF matrix. Turn this off if you want to prevent words in\n",
    "                   topic representations that could not be found in the documents at timestamp t.\n",
    "\n",
    "    Returns:\n",
    "        topics_over_time: A dataframe that contains the topic, words, and frequency of topic\n",
    "                              at timestamp t.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    The timestamps variable represent the timestamp of each document. If you have over\n",
    "    100 unique timestamps, it is advised to bin the timestamps as shown below:\n",
    "\n",
    "    ```python\n",
    "    from bertopic import BERTopic\n",
    "    topic_model = BERTopic()\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    topics_over_time = topic_model.topics_over_time(docs, topics, timestamps, nr_bins=20)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    check_is_fitted(self)\n",
    "    check_documents_type(docs)\n",
    "    documents = pd.DataFrame({\"Document\": docs, \"Topic\": topics, \"Timestamps\": timestamps, \"Upvotes\": upvotes})\n",
    "    global_c_tf_idf = normalize(self.c_tf_idf, axis=1, norm='l1', copy=False)\n",
    "\n",
    "    all_topics = sorted(list(documents.Topic.unique()))\n",
    "    all_topics_indices = {topic: index for index, topic in enumerate(all_topics)}\n",
    "\n",
    "    if isinstance(timestamps[0], str):\n",
    "        infer_datetime_format = True if not datetime_format else False\n",
    "        documents[\"Timestamps\"] = pd.to_datetime(documents[\"Timestamps\"],\n",
    "                                                 infer_datetime_format=infer_datetime_format,\n",
    "                                                 format=datetime_format)\n",
    "\n",
    "    if nr_bins:\n",
    "        documents[\"Bins\"] = pd.cut(documents.Timestamps, bins=nr_bins)\n",
    "        documents[\"Timestamps\"] = documents.apply(lambda row: row.Bins.left, 1)\n",
    "\n",
    "    # Sort documents in chronological order\n",
    "    documents = documents.sort_values(\"Timestamps\")\n",
    "    timestamps = documents.Timestamps.unique()\n",
    "    if len(timestamps) > 100:\n",
    "        warnings.warn(f\"There are more than 100 unique timestamps (i.e., {len(timestamps)}) \"\n",
    "                      \"which significantly slows down the application. Consider setting `nr_bins` \"\n",
    "                      \"to a value lower than 100 to speed up calculation. \")\n",
    "\n",
    "    # For each unique timestamp, create topic representations\n",
    "    topics_over_time = []\n",
    "    for index, timestamp in tqdm(enumerate(timestamps), disable=not self.verbose):\n",
    "\n",
    "        # Calculate c-TF-IDF representation for a specific timestamp\n",
    "        selection = documents.loc[documents.Timestamps == timestamp, :]\n",
    "        documents_per_topic = selection.groupby(['Topic'], as_index=False).agg({'Document': ' '.join,\n",
    "                                                                                \"Timestamps\": \"count\",\n",
    "                                                                                \"Upvotes\": np.sum})\n",
    "        c_tf_idf, words = self._c_tf_idf(documents_per_topic, fit=False)\n",
    "\n",
    "        if global_tuning or evolution_tuning:\n",
    "            c_tf_idf = normalize(c_tf_idf, axis=1, norm='l1', copy=False)\n",
    "\n",
    "        # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF\n",
    "        # matrix at timestamp t-1\n",
    "        if evolution_tuning and index != 0:\n",
    "            current_topics = sorted(list(documents_per_topic.Topic.values))\n",
    "            overlapping_topics = sorted(list(set(previous_topics).intersection(set(current_topics))))\n",
    "\n",
    "            current_overlap_idx = [current_topics.index(topic) for topic in overlapping_topics]\n",
    "            previous_overlap_idx = [previous_topics.index(topic) for topic in overlapping_topics]\n",
    "\n",
    "            c_tf_idf.tolil()[current_overlap_idx] = ((c_tf_idf[current_overlap_idx] +\n",
    "                                                          previous_c_tf_idf[previous_overlap_idx]) / 2.0).tolil()\n",
    "\n",
    "        # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation\n",
    "        # by simply taking the average of the two\n",
    "        if global_tuning:\n",
    "            selected_topics = [all_topics_indices[topic] for topic in documents_per_topic.Topic.values]\n",
    "            c_tf_idf = (global_c_tf_idf[selected_topics] + c_tf_idf) / 2.0\n",
    "\n",
    "        # Extract the words per topic\n",
    "        labels = sorted(list(documents_per_topic.Topic.unique()))\n",
    "        words_per_topic = self._extract_words_per_topic(words, c_tf_idf, labels)\n",
    "        topic_frequency = pd.Series(documents_per_topic.Upvotes.values,\n",
    "                                    index=documents_per_topic.Topic).to_dict()\n",
    "\n",
    "        # Fill dataframe with results\n",
    "        topics_at_timestamp = [(topic,\n",
    "                                \", \".join([words[0] for words in values][:6]),\n",
    "                                topic_frequency[topic],\n",
    "                                timestamp) for topic, values in words_per_topic.items()]\n",
    "        topics_over_time.extend(topics_at_timestamp)\n",
    "\n",
    "        if evolution_tuning:\n",
    "            previous_topics = sorted(list(documents_per_topic.Topic.values))\n",
    "            previous_c_tf_idf = c_tf_idf.copy()\n",
    "\n",
    "    return pd.DataFrame(topics_over_time, columns=[\"Topic\", \"Words\", \"Frequency\", \"Timestamp\"])\n",
    "\n",
    "topic_model.topics_over_time_upvotes = MethodType(topics_over_time_upvotes, topic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21341bdb",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73dbc10",
   "metadata": {},
   "source": [
    "### Topics from 2020-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86576df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for each document\n",
    "embeddings = topic_model._extract_embeddings(docs, method='document')\n",
    "\n",
    "# Dimensionality reduction to 2D\n",
    "umap_model_2d = UMAP(n_neighbors=28, n_components=2, min_dist=0.0, metric='cosine').fit(embeddings)\n",
    "\n",
    "# Combine the data\n",
    "plot_df = pd.DataFrame(umap_model_2d.embedding_, columns=['x', 'y'])\n",
    "plot_df['topic'] = topics\n",
    "\n",
    "# Select the data\n",
    "top_n = 20\n",
    "plot_df = plot_df.loc[plot_df.topic < top_n]\n",
    "\n",
    "# Plot 2D embeddings\n",
    "fig, ax = plt.subplots(figsize=(21,15))\n",
    "\n",
    "# Plot outliers (-1)\n",
    "ax.scatter(\n",
    "    plot_df.loc[plot_df['topic'] == -1, 'x'],\n",
    "    plot_df.loc[plot_df['topic'] == -1, 'y'],\n",
    "    c='whitesmoke',\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "# Plot inliers (0 to top_n-1)\n",
    "scatter = ax.scatter(\n",
    "    plot_df.loc[plot_df['topic'] != -1, 'x'],\n",
    "    plot_df.loc[plot_df['topic'] != -1, 'y'],\n",
    "    c=plot_df.loc[plot_df['topic'] != -1, 'topic'], \n",
    "    alpha=0.5, \n",
    "    cmap='tab20b'\n",
    ")\n",
    "\n",
    "# Generate colored legend\n",
    "topic_labels = topic_model.get_topic_info()['Name'].to_list()[1:top_n+1]\n",
    "handles, _ = scatter.legend_elements(num=None, alpha=1)\n",
    "legend1 = ax.legend(handles, topic_labels, ncol=1, loc='upper left', title=\"Tópicos\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106a153",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate intertopic distance map for all topics\n",
    "# vis_distmap_all = topic_model.visualize_topics(topics=topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56185b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_distmap_all.write_html('./vis_distmap_all.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dendrogram for all topics\n",
    "vis_hierarchy_all = topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hierarchy_all.write_html('./vis_hierarchy_all.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5737308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate barchart for observed pandemic-related topics\n",
    "vis_barchart = topic_model.visualize_barchart(topics=[29, 37, 64, 90, 100, 106, 123, 127, 139, 141, 156, 159, 160], n_words=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_barchart.write_html('./vis_barchart.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c3382",
   "metadata": {},
   "source": [
    "### Topics from 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate intertopic distance map for 2020 topics\n",
    "# vis_distmap_y1 = topic_model.visualize_topics(topics=y1_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_distmap_y1.write_html('./vis_distmap_y1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793265bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate dendrogram for 2020 topics\n",
    "# vis_hierarchy_y1 = topic_model.visualize_hierarchy(topics=y1_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_hierarchy_y1.write_html('./vis_hierarchy_y1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009620c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_topics_over_time_upvotes = topic_model.topics_over_time_upvotes(docs=y1_docs,\n",
    "                                                                   topics=y1_topics,\n",
    "                                                                   timestamps=y1_timestamps,\n",
    "                                                                   upvotes=y1_upvotes,\n",
    "                                                                   global_tuning=False,\n",
    "                                                                   evolution_tuning=True,\n",
    "                                                                   nr_bins=12)\n",
    "\n",
    "topics_over_time_upvotes_y1 = topic_model.visualize_topics_over_time(y1_topics_over_time_upvotes, top_n_topics=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time_upvotes_y1.write_html('./vis_topics_over_time_upvotes_y1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc90727",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(topic=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc952d",
   "metadata": {},
   "source": [
    "### Topics from 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10889a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate intertopic distance map for 2021 topics\n",
    "# vis_distmap_y2 = topic_model.visualize_topics(topics=y2_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_distmap_y2.write_html('./vis_distmap_y2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4189a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate dendrogram for 2021 topics\n",
    "# vis_hierarchy_y2 = topic_model.visualize_hierarchy(topics=y2_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_hierarchy_y2.write_html('./vis_hierarchy_y2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_topics_over_time_upvotes = topic_model.topics_over_time_upvotes(docs=y2_docs,\n",
    "                                                                   topics=y2_topics,\n",
    "                                                                   timestamps=y2_timestamps,\n",
    "                                                                   upvotes=y2_upvotes,\n",
    "                                                                   global_tuning=False,\n",
    "                                                                   evolution_tuning=True,\n",
    "                                                                   nr_bins=12)\n",
    "\n",
    "topics_over_time_upvotes_y2 = topic_model.visualize_topics_over_time(y2_topics_over_time_upvotes, top_n_topics=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49feca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time_upvotes_y2.write_html('./vis_topics_over_time_upvotes_y2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f27f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(topic=65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
